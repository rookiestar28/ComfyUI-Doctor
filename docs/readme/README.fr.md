# ComfyUI-Doctor

[ÁπÅ‰∏≠](README.zh-TW.md) | [ÁÆÄ‰∏≠](README.zh-CN.md) | [Êó•Êú¨Ë™û](README.ja.md) | [ÌïúÍµ≠Ïñ¥](README.ko.md) | [Deutsch](README.de.md) | Fran√ßais | [Italiano](README.it.md) | [Espa√±ol](README.es.md) | [English](../README.md) | [Roadmap & √âtat du d√©veloppement](../ROADMAP.md)

Une suite de diagnostics d'ex√©cution continue et en temps r√©el pour ComfyUI comprenant **une analyse aliment√©e par LLM**, **un chat de d√©bogage interactif** et **plus de 50 mod√®les de correction**. Intercepte automatiquement toutes les sorties du terminal d√®s le d√©marrage, capture des traces Python compl√®tes (tracebacks) et fournit des suggestions de correction prioris√©es avec extraction de contexte au niveau du n≈ìud. Prend d√©sormais en charge la **gestion des motifs bas√©e sur JSON** avec rechargement √† chaud et **prise en charge i18n compl√®te** pour 9 langues (en, zh_TW, zh_CN, ja, de, fr, it, es, ko).

## Derni√®res mises √† jour (Jan 2026) - Cliquer pour d√©velopper

<details>
<summary><strong>Nouvelle fonctionnalit√© : F14 Diagnostics Proactifs (Bilan de Sant√© + Signature d'Intention)</strong></summary>

- Une section **Diagnostics (Diagnostics)** a √©t√© ajout√©e √† l'onglet **Statistiques (Statistics)** pour d√©panner de mani√®re proactive les probl√®mes de flux de travail (sans LLM).
- **Bilan de Sant√© (Health Check)** : Comprend le contr√¥le des flux de travail (lint), des actifs d'environnement (env assets) et des contr√¥les de confidentialit√©, et fournit des suggestions de correction exploitables.
- **Signature d'Intention (Intent Signature)** : Syst√®me d'inf√©rence d'intention d√©terministe, fournissant des **Intentions Top-K + Preuves** pour aider √† d√©terminer ce que le flux de travail "essaie de faire".
- Comprend le renforcement de l'UX : Replis s√©curis√©s (par ex. "Aucune intention dominante d√©tect√©e") et m√©canismes am√©lior√©s d'assainissement des preuves.

</details>

<details>
<summary><strong>(v1.5.8) QoL: Auto-open Right Error Report Panel Toggle</strong></summary>

- Added a **dedicated toggle** in **Doctor ‚Üí Settings** to control whether the **right-side error report panel** auto-opens when a new error is detected.
- **Default: ON** for new installs, and the choice is persisted.

</details>

<details>
<summary><strong>Gestion Intelligente du Budget de Jetons (v1.5.0)</strong></summary>

**Gestion Contextuelle Intelligente (Optimisation des Co√ªts) :**

- **D√©coupage automatique** : Pour les LLM distants (r√©duction de 60-80% des jetons)
- **Strat√©gie progressive** : √âlagage du workflow ‚Üí suppression des infos syst√®me ‚Üí troncature de la trace
- **Opt-in Local** : D√©coupage l√©ger pour Ollama/LMStudio (limites de 12K/16K)
- **Observabilit√© Am√©lior√©e** : Suivi des jetons √©tape par √©tape & Outil de validation A/B

**R√©silience R√©seau :**

- **Backoff Exponentiel** : R√©essai automatique pour erreurs 429/5xx avec jitter
- **Protection du Streaming** : Watchdog de 30s pour chunks SSE bloqu√©s
- **Limites de D√©bit & Concurrence** : Token bucket (30/min) + S√©maphore de concurrence (max 3)

**Nouvelle Configuration :**

| Config Key | Default | Description |
|------------|---------|-------------|
| `r12_enabled_remote` | `true` | Activer le budget intelligent (Remote) |
| `retry_max_attempts` | `3` | Max r√©essais |
| `stream_chunk_timeout` | `30` | Timeout de flux (sec) |

</details>

<details>
<summary><strong>Correctif Majeur: Gouvernance du Pipeline & S√©curit√© des Plugins (v1.4.5)</strong></summary>

**Renforcement de la S√©curit√© :**

- **Protection SSRF++** : Remplacement des v√©rifications de sous-cha√Ænes par une analyse Host/Port appropri√©e ; blocage des redirections sortantes (`allow_redirects=False`)
- **Entonnoir de Nettoyage Sortant** : Une limite unique (`outbound.py`) garantit le nettoyage de TOUTES les charges utiles externes ; `privacy_mode=none` autoris√© uniquement pour les LLM locaux v√©rifi√©s

**Syst√®me de Confiance des Plugins :**

- **S√©curis√© par d√©faut** : Plugins d√©sactiv√©s par d√©faut, n√©cessitent une liste d'autorisation explicite (Allowlist) + Manifeste/SHA256
- **Classification de Confiance** : `trusted` (approuv√©) | `unsigned` (non sign√©) | `untrusted` (non approuv√©) | `blocked` (bloqu√©)
- **Confinement du Syst√®me de Fichiers** : Confinement par realpath, refus des liens symboliques, limites de taille, r√®gles strictes de nom de fichier
- **Signature HMAC Optionnelle** : V√©rification de l'int√©grit√© par secret partag√© (pas de signature √† cl√© publique)

**Gouvernance du Pipeline :**

- **Contrats de M√©tadonn√©es** : Versionnage de sch√©ma + validation post-ex√©cution + Quarantaine pour les cl√©s invalides
- **Politique de D√©pendance** : Application de `requires/provides` ; d√©pendance manquante ‚Üí √©tape ignor√©e, statut `degraded` (d√©grad√©)
- **Contre-pression du Logger** : `DroppingQueue` avec gestion des priorit√©s + m√©triques de rejet
- **Transfert avant d√©marrage** : D√©sinstallation propre du Logger avant la prise en charge par SmartLogger

**Observabilit√© :**

- Point de terminaison `/doctor/health` : Expose les m√©triques de file d'attente, les comptes de rejets, les blocages SSRF et le statut du pipeline

**R√©sultats des Tests** : 159 tests Python r√©ussis | 17 tests de Gate Phase 2

</details>

<details>
<summary><strong>Am√©lioration: CI Gates & Outillage Plugins</strong></summary>

**T11 - CI Gate de Version Phase 2 :**

- Workflow GitHub Actions (`phase2-release-gate.yml`) : Impose 4 suites pytest + E2E
- Script de validation locale (`scripts/phase2_gate.py`) : Supporte les modes `--fast` et `--e2e`

**T12 - V√©rificateur Statique de S√©curit√© Sortante :**

- Analyseur bas√© sur AST (`scripts/check_outbound_safety.py`) d√©tecte les mod√®les de contournement
- 6 r√®gles de d√©tection : `RAW_FIELD_IN_PAYLOAD`, `DANGEROUS_FALLBACK`, `POST_WITHOUT_SANITIZATION`, etc.
- Workflow CI + 8 tests unitaires + Documentation (`docs/OUTBOUND_SAFETY.md`)

**A8 - Outillage de Migration de Plugin :**

- `scripts/plugin_manifest.py` : G√©n√®re un manifeste avec des hachages SHA256
- `scripts/plugin_allowlist.py` : Scanne les plugins et sugg√®re une configuration
- `scripts/plugin_validator.py` : Valide le manifeste et la configuration
- `scripts/plugin_hmac_sign.py` : G√©n√®re des signatures HMAC optionnelles
- Documentation mise √† jour : `docs/PLUGIN_MIGRATION.md`, `docs/PLUGIN_GUIDE.md`

</details>

<details>
<summary><strong>Am√©lioration: Docs CSP & T√©l√©m√©trie</strong></summary>

**S1 - Docs de Conformit√© CSP :**

- V√©rification que tous les actifs se chargent localement (`web/lib/`) ; les URL CDN sont uniquement en secours
- Ajout de la section "CSP Compatibility" au README
- Audit de code termin√© (en attente de v√©rification manuelle)

**S3 - Infrastructure de T√©l√©m√©trie Locale :**

- Backend : `telemetry.py` (TelemetryStore, RateLimiter, d√©tection PII)
- 6 Points de Terminaison API : `/doctor/telemetry/{status,buffer,track,clear,export,toggle}`
- Frontend : Contr√¥les UI des param√®tres pour la gestion de la t√©l√©m√©trie
- S√©curit√© : V√©rification de l'origine (403 Cross-Origin), limite de charge utile 1KB, liste blanche de champs
- **D√©sactiv√© par d√©faut** : Aucun enregistrement/R√©seau sauf activation explicite
- 81 cha√Ænes i18n (9 cl√©s √ó 9 langues)

**R√©sultats des Tests** : 27 Tests Unitaires T√©l√©m√©trie | 8 Tests E2E

</details>

<details>
<summary><strong>Am√©lioration : Renforcement Runner E2E & UI Confiance/Sant√©</strong></summary>

**Renforcement Runner E2E (Support WSL `/mnt/c`) :**

- Correction des probl√®mes de permission du cache de traduction Playwright sur WSL
- Ajout d'un r√©pertoire temporaire accessible en √©criture (`.tmp/playwright`) sous le repo
- Remplacement `PW_PYTHON` pour la compatibilit√© multiplateforme

**Panneau UI Confiance & Sant√© :**

- Ajout du panneau "Trust & Health" √† l'onglet Statistiques (Statistics)
- Affiche : pipeline_status, ssrf_blocked, dropped_logs
- Liste de confiance des plugins (avec badges et raisons)
- Point de terminaison de scan uniquement `GET /doctor/plugins` (pas d'importation de code)

**R√©sultats des Tests** : 61/61 Tests E2E R√©ussis | 159/159 Tests Python R√©ussis

</details>

<details>
<summary><strong>Mises √† jour pr√©c√©dentes (v1.4.0, Jan 2026)</strong></summary>

- Migration A7 Preact Termin√©e (Phase 5A‚Äì5C : √élots Chat/Stats, registre, rendu partag√©, solutions de repli robustes).
- Renforcement de l'Int√©gration : Couverture Playwright E2E renforc√©e.
- Correctifs UI : Correction du timing de l'infobulle de la barre lat√©rale.

</details>

<details>
<summary><strong>Tableau de bord statistiques</strong></summary>

**Suivez votre stabilit√© ComfyUI en un coup d'≈ìil !**

ComfyUI-Doctor inclut d√©sormais un **Tableau de bord statistiques** qui fournit des informations sur les tendances d'erreurs, les probl√®mes courants et la progression de la r√©solution.

**Fonctionnalit√©s** :

- üìä **Tendances d'erreurs** : Suivez les erreurs sur 24h/7j/30j
- üî• **Top 5 des motifs** : Voyez quelles erreurs se produisent le plus fr√©quemment
- üìà **R√©partition par cat√©gorie** : Visualisez les erreurs par cat√©gorie (M√©moire, Flux de travail, Chargement de mod√®le, etc.)
- ‚úÖ **Suivi de r√©solution** : Surveillez les erreurs r√©solues vs non r√©solues
- üåç **Support i18n complet** : Disponible dans les 9 langues

![Tableau de bord statistiques](../../assets/statistics_panel.png)

**Comment utiliser** :

1. Ouvrez le panneau lat√©ral Doctor (cliquez sur l'ic√¥ne üè• √† gauche)
2. D√©veloppez la section "üìä Statistiques d'erreurs"
3. Consultez les analyses et tendances d'erreurs en temps r√©el
4. Marquez les erreurs comme r√©solues/ignor√©es pour suivre vos progr√®s

**API Backend** :

- `GET /doctor/statistics?time_range_days=30` - R√©cup√©rer les statistiques
- `POST /doctor/mark_resolved` - Mettre √† jour le statut de r√©solution

**Couverture des tests** : 17/17 tests backend ‚úÖ | 14/18 tests E2E (taux de r√©ussite 78%)

**D√©tails de mise en ≈ìuvre** : Voir `.planning/260104-F4_STATISTICS_RECORD.md`

</details>

<details>
<summary><strong>CI de validation des motifs</strong></summary>

**Des contr√¥les qualit√©s automatis√©s prot√®gent d√©sormais l'int√©grit√© des motifs !**

ComfyUI-Doctor inclut d√©sormais des **tests d'int√©gration continue** pour tous les motifs d'erreur, garantissant des contributions z√©ro d√©faut.

**Ce que T8 valide** :

- ‚úÖ **Format JSON** : Les 8 fichiers de motifs se compilent correctement
- ‚úÖ **Syntaxe Regex** : Les 57 motifs ont des expressions r√©guli√®res valides
- ‚úÖ **Compl√©tude i18n** : Couverture de traduction √† 100% (57 motifs √ó 9 langues = 513 v√©rifications)
- ‚úÖ **Conformit√© Sch√©ma** : Champs requis (`id`, `regex`, `error_key`, `priority`, `category`)
- ‚úÖ **Qualit√© M√©tadonn√©es** : Plages de priorit√© valides (50-95), ID uniques, cat√©gories correctes

**Int√©gration GitHub Actions** :

- Se d√©clenche √† chaque push/PR affectant `patterns/`, `i18n.py` ou les tests
- S'ex√©cute en ~3 secondes pour 0 $ (niveau gratuit GitHub Actions)
- Bloque les fusions si la validation √©choue

**Pour les contributeurs** :

```bash
# Validation locale avant commit
python scripts/run_pattern_tests.py

# Sortie :
‚úÖ All 57 patterns have required fields
‚úÖ All 57 regex patterns compile successfully
‚úÖ en: All 57 patterns have translations
‚úÖ zh_TW: All 57 patterns have translations
... (9 langues au total)
```

**R√©sultats des tests** : Taux de r√©ussite de 100% sur tous les contr√¥les

**D√©tails de mise en ≈ìuvre** : Voir `.planning/260103-T8_IMPLEMENTATION_RECORD.md`

</details>

<details>
<summary><strong>Refonte du syst√®me de motifs (√âTAPE 1-3 Termin√©e)</strong></summary>

ComfyUI-Doctor a subi une mise √† niveau architecturale majeure avec **plus de 57 motifs d'erreur** et une **gestion des motifs bas√©e sur JSON** !

**√âTAPE 1 : Correctif de l'architecture Logger**

- Impl√©mentation de SafeStreamWrapper avec traitement en arri√®re-plan bas√© sur une file d'attente
- √âlimination des risques de blocage (deadlock) et des conditions de concurrence (race conditions)
- Correction des conflits d'interception de logs avec le LogInterceptor de ComfyUI

**√âTAPE 2 : Gestion des motifs JSON (F2)**

- Nouveau PatternLoader avec capacit√© de rechargement √† chaud (pas de red√©marrage n√©cessaire !)
- Les motifs sont maintenant d√©finis dans des fichiers JSON sous le r√©pertoire `patterns/`
- 22 motifs int√©gr√©s dans `patterns/builtin/core.json`
- Facile √† √©tendre et √† maintenir

**√âTAPE 3 : Extension des motifs communautaires (F12)**

- **35 nouveaux motifs communautaires** couvrant les extensions populaires :
  - **ControlNet** (8 motifs) : Chargement de mod√®le, pr√©-traitement, dimensionnement d'image
  - **LoRA** (6 motifs) : Erreurs de chargement, compatibilit√©, probl√®mes de poids
  - **VAE** (5 motifs) : √âchec d'encodage/d√©codage, pr√©cision, tuilage
  - **AnimateDiff** (4 motifs) : Chargement de mod√®le, nombre de trames, longueur de contexte
  - **IPAdapter** (4 motifs) : Chargement de mod√®le, encodage d'image, compatibilit√©
  - **FaceRestore** (3 motifs) : Mod√®les CodeFormer/GFPGAN, d√©tection
  - **Divers** (5 motifs) : Checkpoints, √©chantillonneurs, planificateurs, CLIP
- Support i18n complet pour l'anglais, le chinois traditionnel et le chinois simplifi√©
- Total : **57 motifs d'erreur** (22 int√©gr√©s + 35 communautaires)

**Avantages** :

- ‚úÖ Couverture d'erreurs plus compl√®te
- ‚úÖ Rechargement √† chaud des motifs sans red√©marrer ComfyUI
- ‚úÖ La communaut√© peut contribuer des motifs via des fichiers JSON
- ‚úÖ Base de code plus propre et maintenable

</details>

<details>
<summary><strong>Mises √† jour pr√©c√©dentes (D√©c 2025)</strong></summary>

### F9 : Extension du support multilingue

Nous avons √©tendu le support linguistique de 4 √† 9 langues ! ComfyUI-Doctor fournit d√©sormais des suggestions d'erreurs en :

- **English** Anglais (en)
- **ÁπÅÈ´î‰∏≠Êñá** Chinois Traditionnel (zh_TW)
- **ÁÆÄ‰Ωì‰∏≠Êñá** Chinois Simplifi√© (zh_CN)
- **Êó•Êú¨Ë™û** Japonais (ja)
- **üÜï Deutsch** Allemand (de)
- **üÜï Fran√ßais** (fr)
- **üÜï Italiano** Italien (it)
- **üÜï Espa√±ol** Espagnol (es)
- **üÜï ÌïúÍµ≠Ïñ¥** Cor√©en (ko)

Les 57 motifs d'erreur sont enti√®rement traduits dans toutes les langues, assurant une qualit√© de diagnostic coh√©rente dans le monde entier.

### F8 : Int√©gration des param√®tres de la barre lat√©rale

Les param√®tres ont √©t√© rationalis√©s ! Configurez Doctor directement depuis la barre lat√©rale :

- Cliquez sur l'ic√¥ne ‚öôÔ∏è dans l'en-t√™te de la barre lat√©rale pour acc√©der √† tous les param√®tres
- S√©lection de la langue (9 langues)
- Changement rapide de fournisseur d'IA (OpenAI, DeepSeek, Groq, Gemini, Ollama, etc.)
- Remplissage automatique de l'URL de base lors du changement de fournisseur
- Gestion de la cl√© API (saisie prot√©g√©e par mot de passe)
- Configuration du nom du mod√®le
- Les param√®tres persistent entre les sessions avec localStorage
- Retour visuel lors de la sauvegarde (‚úÖ Enregistr√© ! / ‚ùå Erreur)

Le panneau Param√®tres de ComfyUI affiche d√©sormais uniquement le commutateur Activer/D√©sactiver - tous les autres param√®tres ont √©t√© d√©plac√©s vers la barre lat√©rale pour une exp√©rience plus propre et int√©gr√©e.

</details>

---

## Fonctionnalit√©s

- **Surveillance automatique des erreurs** : Capture toutes les sorties du terminal et d√©tecte les traces Python en temps r√©el
- **Analyse intelligente des erreurs** : Plus de 57 motifs d'erreur (22 int√©gr√©s + 35 communautaires) avec des suggestions exploitables
- **Extraction du contexte du n≈ìud** : Identifie quel n≈ìud a caus√© l'erreur (ID du n≈ìud, Nom, Classe)
- **Contexte de l'environnement syst√®me** : Inclut automatiquement la version Python, les paquets install√©s (pip list) et les informations OS dans l'analyse IA
- **Support multilingue** : 9 langues prises en charge (Anglais, Chinois Traditionnel, Chinois Simplifi√©, Japonais, Allemand, Fran√ßais, Italien, Espagnol, Cor√©en)
- **Gestion des motifs bas√©e sur JSON** : Rechargement √† chaud des motifs d'erreur sans red√©marrer ComfyUI
- **Support des motifs communautaires** : Couvre ControlNet, LoRA, VAE, AnimateDiff, IPAdapter, FaceRestore, et plus
- **N≈ìud inspecteur de d√©bogage** : Inspection approfondie des donn√©es circulant dans votre flux de travail
- **Historique des erreurs** : Maintient un tampon des erreurs r√©centes via l'API
- **API RESTful** : Sept points de terminaison pour l'int√©gration frontend
- **Analyse aliment√©e par IA** : Analyse d'erreur LLM en un clic avec support de plus de 8 fournisseurs (OpenAI, DeepSeek, Groq, Gemini, Ollama, LMStudio, et plus)
- **Interface de chat interactive** : Assistant de d√©bogage IA multi-tours int√©gr√© dans la barre lat√©rale de ComfyUI
- **Interface lat√©rale interactive** : Panneau d'erreur visuel avec localisation du n≈ìud et diagnostics instantan√©s
- **Configuration flexible** : Panneau de configuration complet pour la personnalisation du comportement

### üÜï Interface de Chat IA

La nouvelle interface de chat interactive offre une exp√©rience de d√©bogage conversationnelle directement dans la barre lat√©rale gauche de ComfyUI. Lorsqu'une erreur survient, cliquez simplement sur "Analyze with AI" pour d√©marrer une conversation multi-tours avec votre LLM pr√©f√©r√©.

<div align="center">
<img src="../../assets/chat-ui.png" alt="Interface de Chat IA">
</div>

**Caract√©ristiques cl√©s :**

- **Conscient du contexte** : Inclut automatiquement les d√©tails de l'erreur, les informations sur le n≈ìud et le contexte du flux de travail
- **Conscient de l'environnement** : Inclut la version Python, les paquets install√©s et les informations OS pour un d√©bogage pr√©cis
- **R√©ponses en streaming** : R√©ponses LLM en temps r√©el avec un formatage correct
- **Conversations multi-tours** : Posez des questions de suivi pour approfondir les probl√®mes
- **Toujours accessible** : La zone de saisie reste visible en bas avec un positionnement collant
- **Supporte plus de 8 fournisseurs LLM** : OpenAI, DeepSeek, Groq, Gemini, Ollama, LMStudio, et plus
- **Mise en cache intelligente** : Liste des paquets mise en cache pendant 24 heures pour √©viter l'impact sur les performances

**Comment utiliser :**

1. Lorsqu'une erreur survient, ouvrez la barre lat√©rale Doctor (panneau de gauche)
2. Cliquez sur le bouton "‚ú® Analyze with AI" dans la zone de contexte d'erreur
3. L'IA analysera automatiquement l'erreur et fournira des suggestions
4. Continuez la conversation en tapant des questions de suivi dans la zone de saisie
5. Appuyez sur Entr√©e ou cliquez sur "Send" pour envoyer votre message

> **üí° Astuce API gratuite** : [Google AI Studio](https://aistudio.google.com/app/apikey) (Gemini) offre un niveau gratuit g√©n√©reux sans carte de cr√©dit requise. Parfait pour commencer avec le d√©bogage aliment√© par IA sans aucuns frais !

---

## Installation

### Option 1 : Utiliser ComfyUI-Manager (Recommand√©)

1. Ouvrez ComfyUI et cliquez sur le bouton **Manager** dans le menu
2. S√©lectionnez **Install Custom Nodes**
3. Recherchez `ComfyUI-Doctor`
4. Cliquez sur **Install** et red√©marrez ComfyUI

### Option 2 : Installation manuelle (Git Clone)

1. Naviguez vers votre r√©pertoire de n≈ìuds personnalis√©s ComfyUI :

   ```bash
   cd ComfyUI/custom_nodes/
   ```

2. Clonez ce d√©p√¥t :

   ```bash
   git clone https://github.com/rookiestar28/ComfyUI-Doctor.git
   ```

3. Red√©marrez ComfyUI

4. Recherchez le message d'initialisation dans la console :

   ```text
   [ComfyUI-Doctor] Initializing Smart Debugger...
   [ComfyUI-Doctor] Log file: .../logs/comfyui_debug_2025-12-28.log
   
   ==================== SYSTEM SNAPSHOT ====================
   OS: Windows 11
   Python: 3.12.3
   PyTorch: 2.0.1+cu118
   CUDA Available: True
   ...
   ```

---

## Utilisation

### Mode Passif (Automatique)

Une fois install√©, ComfyUI-Doctor fait automatiquement :

- Enregistre toutes les sorties de la console dans le r√©pertoire `logs/`
- D√©tecte les erreurs et fournit des suggestions
- Enregistre les informations sur l'environnement syst√®me

**Exemple de sortie d'erreur** :

```
Traceback (most recent call last):
  ...
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB

----------------------------------------
ERROR LOCATION: Node ID: #42 | Name: KSampler
SUGGESTION: OOM (Out Of Memory) : Votre VRAM GPU est pleine. Essayez :
   1. R√©duire la taille du lot (Batch Size)
   2. Utiliser le drapeau '--lowvram'
   3. Fermer d'autres applications GPU
----------------------------------------
```

### Mode Actif (N≈ìud de d√©bogage)

1. Faites un clic droit sur le canevas ‚Üí `Add Node` ‚Üí `Smart Debug Node`
2. Connectez le n≈ìud en ligne avec n'importe quelle connexion (prend en charge l'entr√©e joker `*`)
3. Ex√©cutez votre flux de travail

**Exemple de sortie** :

```text
[DEBUG] Data Inspection:
  Type: Tensor
  Shape: torch.Size([1, 4, 64, 64])
  Dtype: torch.float16
  Device: cuda:0
  Stats (All): Min=-3.2156, Max=4.8912, Mean=0.0023
```

Le n≈ìud transmet les donn√©es sans affecter l'ex√©cution du flux de travail.

---

## Interface Utilisateur (UI) Frontend

ComfyUI-Doctor fournit une interface de barre lat√©rale interactive pour la surveillance des erreurs et les diagnostics en temps r√©el.

### Acc√©der au panneau Doctor

Cliquez sur le bouton **üè• Doctor** dans le menu ComfyUI (barre lat√©rale gauche) pour ouvrir le panneau Doctor. Le panneau glisse depuis le c√¥t√© droit de l'√©cran.

### Caract√©ristiques de l'interface

<div align="center">
<img src="../../assets/doctor-side-bar.png" alt="Rapport d'erreur">
</div>

L'interface Doctor se compose de deux panneaux :

#### Panneau lat√©ral gauche (Barre lat√©rale Doctor)

Cliquez sur l'ic√¥ne **üè• Doctor** dans le menu gauche de ComfyUI pour acc√©der √† :

- **Panneau de configuration** (ic√¥ne ‚öôÔ∏è) : Configurez la langue, le fournisseur d'IA, les cl√©s API et la s√©lection de mod√®le
- **Carte de contexte d'erreur** : Lorsqu'une erreur survient, affiche :
  - **üí° Suggestion** : Conseil concis et exploitable (ex. "V√©rifiez les connexions d'entr√©e et assurez-vous que les exigences du n≈ìud sont satisfaites.")
  - **Horodatage** : Moment o√π l'erreur s'est produite
  - **Contexte du n≈ìud** : ID et nom du n≈ìud (si applicable)
  - **‚ú® Analyze with AI** : Lancez un chat interactif pour un d√©bogage d√©taill√©
- **Interface de chat IA** : Conversation multi-tours avec votre LLM pour une analyse approfondie des erreurs
- **Zone de saisie collante** : Toujours accessible en bas pour les questions de suivi

#### Panneau d'erreur droit (Dernier diagnostic)

Notifications d'erreur en temps r√©el dans le coin sup√©rieur droit :

![Rapport d'erreur Doctor](../../assets/error-report.png)

- **Indicateur d'√©tat** : Point color√© montrant la sant√© du syst√®me
  - üü¢ **Vert** : Syst√®me fonctionnant normalement, aucune erreur d√©tect√©e
  - üî¥ **Rouge (pulsant)** : Erreur active d√©tect√©e
- **Carte du dernier diagnostic** : Affiche l'erreur la plus r√©cente avec :
  - **R√©sum√© de l'erreur** : Br√®ve description de l'erreur (th√®me rouge, pliable pour les erreurs longues)
  - **üí° Suggestion** : Conseil concis et exploitable (th√®me vert)
  - **Horodatage** : Moment o√π l'erreur s'est produite
  - **Contexte du n≈ìud** : ID, nom et classe du n≈ìud
  - **üîç Localiser le n≈ìud sur le canevas** : Centre et met en √©vidence automatiquement le n≈ìud probl√©matique

**Principes de conception cl√©s** :

- ‚úÖ **Suggestions concises** : Seul le conseil exploitable est affich√© (ex. "V√©rifiez les connexions d'entr√©e...") au lieu de descriptions d'erreurs verbeuses
- ‚úÖ **S√©paration visuelle** : Les messages d'erreur (rouge) et les suggestions (vert) sont clairement distingu√©s
- ‚úÖ **Troncature intelligente** : Les erreurs longues affichent les 3 premi√®res + 3 derni√®res lignes avec d√©tails complets pliables
- ‚úÖ **Mises √† jour en temps r√©el** : Les deux panneaux se mettent √† jour automatiquement lorsque de nouvelles erreurs surviennent via des √©v√©nements WebSocket

---

## Analyse d'erreur aliment√©e par IA

ComfyUI-Doctor s'int√®gre aux services LLM populaires pour fournir des suggestions de d√©bogage intelligentes et contextuelles.

### Fournisseurs d'IA pris en charge

#### Services Cloud

- **OpenAI** (GPT-4, GPT-4o, etc.)
- **DeepSeek** (DeepSeek-V2, DeepSeek-Coder)
- **Groq Cloud** (Llama 3, Mixtral - inf√©rence LPU ultra-rapide)
- **Google Gemini** (Gemini Pro, Gemini Flash)
- **xAI Grok** (Grok-2, Grok-beta)
- **OpenRouter** (Acc√®s √† Claude, GPT-4 et plus de 100 mod√®les)

#### Services Locaux (Aucune cl√© API requise)

- **Ollama** (`http://127.0.0.1:11434`) - Ex√©cutez Llama, Mistral, CodeLlama localement
- **LMStudio** (`http://localhost:1234/v1`) - Inf√©rence de mod√®le locale avec GUI

> **üí° Compatibilit√© multiplateforme** : Les URL par d√©faut peuvent √™tre remplac√©es via des variables d'environnement :
>
> - `OLLAMA_BASE_URL` - Point de terminaison Ollama personnalis√© (d√©faut : `http://127.0.0.1:11434`)
> - `LMSTUDIO_BASE_URL` - Point de terminaison LMStudio personnalis√© (d√©faut : `http://localhost:1234/v1`)
>
> Cela √©vite les conflits entre les instances Ollama Windows et WSL2, ou lors de l'ex√©cution dans des configurations Docker/personnalis√©es.

### Configuration

![Panneau de configuration](../../assets/settings.png)

Configurez l'analyse IA dans le panneau **Barre lat√©rale Doctor** ‚Üí **Settings** :

1. **AI Provider** : S√©lectionnez dans le menu d√©roulant. L'URL de base se remplira automatiquement.
2. **AI Base URL** : Le point de terminaison API (auto-peupl√©, mais personnalisable)
3. **AI API Key** : Votre cl√© API (laissez vide pour les LLM locaux comme Ollama/LMStudio)
4. **AI Model Name** :
   - S√©lectionnez un mod√®le dans la liste d√©roulante (automatiquement peupl√©e depuis l'API de votre fournisseur)
   - Cliquez sur le bouton d'actualisation üîÑ pour recharger les mod√®les disponibles
   - Ou cochez "Saisir le nom du mod√®le manuellement" pour taper un nom de mod√®le personnalis√©
5. **Mode de confidentialit√©** : S√©lectionnez le niveau de d√©sinfection PII pour les services d'IA cloud (voir la section [Mode de confidentialit√© (D√©sinfection PII)](#mode-de-confidentialit√©-d√©sinfection-pii) ci-dessous pour plus de d√©tails)

### Utilisation de l'analyse IA

1. Le panneau Doctor s'ouvre automatiquement lorsqu'une erreur survient.
2. Examinez les suggestions int√©gr√©es, ou cliquez sur le bouton ‚ú® Analyze with AI sur la carte d'erreur.
3. Attendez que le LLM analyse l'erreur (g√©n√©ralement 3-10 secondes).
4. Examinez les suggestions de d√©bogage g√©n√©r√©es par l'IA.

**Note de s√©curit√©** : Votre cl√© API est transmise de mani√®re s√©curis√©e du frontend au backend uniquement pour la demande d'analyse. Elle n'est jamais enregistr√©e ou stock√©e de mani√®re persistante.

### Mode de confidentialit√© (D√©sinfection PII)

ComfyUI-Doctor inclut une **d√©sinfection automatique des PII (Informations Personnellement Identifiables)** pour prot√©ger votre vie priv√©e lors de l'envoi de messages d'erreur aux services d'IA cloud.

**Trois niveaux de confidentialit√©** :

| Niveau | Description | Ce qui est supprim√© | Recommand√© pour |
| ----- | ----------- | --------------- | --------------- |
| **None** | Aucune d√©sinfection | Rien | LLM Locaux (Ollama, LMStudio) |
| **Basic** (D√©faut) | Protection standard | Chemins utilisateur, cl√©s API, emails, adresses IP | La plupart des utilisateurs avec des LLM Cloud |
| **Strict** | Confidentialit√© maximale | Tout de Basic + IPv6, empreintes SSH | Exigences Entreprise/Conformit√© |

**Ce qui est d√©sinfect√©** (Niveau Basic) :

- ‚úÖ Chemins utilisateur Windows : `C:\Users\john\file.py` ‚Üí `<USER_PATH>\file.py`
- ‚úÖ Accueil Linux/macOS : `/home/alice/test.py` ‚Üí `<USER_HOME>/test.py`
- ‚úÖ Cl√©s API : `sk-abc123...` ‚Üí `<API_KEY>`
- ‚úÖ Adresses email : `user@example.com` ‚Üí `<EMAIL>`
- ‚úÖ IPs priv√©es : `192.168.1.1` ‚Üí `<PRIVATE_IP>`
- ‚úÖ Identifiants URL : `https://user:pass@host` ‚Üí `https://<USER>@host`

**Ce qui n'est PAS supprim√©** :

- ‚ùå Messages d'erreur (n√©cessaires pour le d√©bogage)
- ‚ùå Noms de mod√®les, noms de n≈ìuds
- ‚ùå Structure du flux de travail
- ‚ùå Chemins de fichiers publics (`/usr/bin/python`)

**Configurer le mode de confidentialit√©** : Ouvrez la barre lat√©rale Doctor ‚Üí Settings ‚Üí Menu d√©roulant üîí Privacy Mode. Les modifications s'appliquent imm√©diatement √† toutes les demandes d'analyse IA.

**Conformit√© RGPD** : Cette fonctionnalit√© prend en charge l'article 25 du RGPD (Protection des donn√©es d√®s la conception) et est recommand√©e pour les d√©ploiements d'entreprise.

### Tableau de bord statistiques

![Panneau statistiques](../../assets/statistics_panel.png)

Le **Tableau de bord statistiques** fournit des informations en temps r√©el sur vos mod√®les d'erreurs ComfyUI et les tendances de stabilit√©.

**Fonctionnalit√©s** :

- **üìä Tendances d'erreurs** : Erreurs totales et comptes pour les derniers 24h/7j/30j
- **üî• Principaux motifs d'erreur** : Les 5 types d'erreurs les plus fr√©quents avec le nombre d'occurrences
- **üìà R√©partition par cat√©gorie** : R√©partition visuelle par cat√©gorie d'erreur (M√©moire, Flux de travail, Chargement de mod√®le, Cadre, G√©n√©rique)
- **‚úÖ Suivi de r√©solution** : Suivez les erreurs r√©solues, non r√©solues et ignor√©es
- **üß≠ Contr√¥les de statut** : Marquer la derni√®re erreur comme R√©solu / Non r√©solu / Ignor√© depuis l‚Äôonglet Statistiques
- **üõ°Ô∏è Confiance et Sant√© (Trust & Health)** : Voir les m√©triques `/doctor/health` et le rapport de confiance des plugins (scan uniquement)
- **üìä T√©l√©m√©trie Anonyme (Anonymous Telemetry) (En construction üöß)** : Opt-in tampon local pour les √©v√©nements d'utilisation (basculer/voir/effacer/exporter)

**Comment utiliser** :

1. Ouvrez la barre lat√©rale Doctor (cliquez sur l'ic√¥ne üè• √† gauche)
2. Trouvez la section pliable **üìä Statistiques d'erreurs**
3. Cliquez pour d√©velopper et afficher vos analyses d'erreurs
4. Utilisez les boutons **Marquer comme** pour d√©finir l‚Äô√©tat de la derni√®re erreur (R√©solu / Non r√©solu / Ignor√©)
5. Faites d√©filer jusqu'au bas de l'onglet Statistiques pour trouver **Confiance et Sant√©** et **T√©l√©m√©trie Anonyme**.

**Contr√¥les du statut** :

- Les boutons ne sont activ√©s que lorsqu‚Äôun horodatage de la derni√®re erreur est disponible
- Les mises √† jour de statut sont conserv√©es dans l‚Äôhistorique et actualisent automatiquement le taux de r√©solution

**Comprendre les donn√©es** :

- **Total (30j)** : Erreurs cumul√©es au cours des 30 derniers jours
- **Derni√®res 24h** : Erreurs au cours des derni√®res 24 heures (aide √† identifier les probl√®mes r√©cents)
- **Taux de r√©solution** : Montre les progr√®s vers la r√©solution des probl√®mes connus
  - üü¢ **R√©solu** : Probl√®mes que vous avez corrig√©s
  - üü† **Non r√©solu** : Probl√®mes actifs n√©cessitant une attention
  - ‚ö™ **Ignor√©** : Probl√®mes non critiques que vous avez choisi d'ignorer
- **Top Motifs** : Identifie quels types d'erreurs n√©cessitent une attention prioritaire
- **Cat√©gories** : Vous aide √† comprendre si les probl√®mes sont li√©s √† la m√©moire, aux flux de travail, aux √©checs de chargement de mod√®le, etc.

**Persistance de l'√©tat du panneau** : L'√©tat ouvert/ferm√© du panneau est enregistr√© dans le localStorage de votre navigateur, de sorte que votre pr√©f√©rence persiste entre les sessions.

### Exemple de configuration de fournisseur

| Fournisseur      | URL de base                                                | Exemple de mod√®le            |
|------------------|------------------------------------------------------------|------------------------------|
| OpenAI           | `https://api.openai.com/v1`                                | `gpt-4o`                     |
| DeepSeek         | `https://api.deepseek.com/v1`                              | `deepseek-chat`              |
| Groq             | `https://api.groq.com/openai/v1`                           | `llama-3.1-70b-versatile`    |
| Gemini           | `https://generativelanguage.googleapis.com/v1beta/openai`  | `gemini-1.5-flash`           |
| Ollama (Local)   | `http://localhost:11434/v1`                                | `llama3.1:8b`                |
| LMStudio (Local) | `http://localhost:1234/v1`                                 | Mod√®le charg√© dans LMStudio  |

---

## Param√®tres

You can also customize ComfyUI-Doctor behavior via the **Doctor sidebar ‚Üí Settings** tab.

### 1. Show error notifications (Afficher les notifications d'erreur)

**Fonction** : Active/d√©sactive les cartes de notification d'erreur flottantes (toasts) dans le coin sup√©rieur droit.
**Utilisation** : D√©sactivez si vous pr√©f√©rez v√©rifier les erreurs manuellement dans la barre lat√©rale sans interruptions visuelles.

### 2. Auto-open panel on error (Ouvrir automatiquement le panneau en cas d'erreur)

**Function**: Automatically opens the **right-side error report panel** when a new error is detected.
**Usage**: **Default: ON**. Disable if you prefer to keep the panel closed and open it manually.

### 3. Error Check Interval (ms)

**Fonction** : Fr√©quence des v√©rifications d'erreur frontend-backend (en millisecondes). D√©faut : `2000`.
**Utilisation** : Des valeurs plus basses (ex. 500) donnent un retour plus rapide mais augmentent la charge ; des valeurs plus √©lev√©es (ex. 5000) √©conomisent les ressources.

### 4. Suggestion Language (Langue de suggestion)

**Fonction** : Langue pour les rapports de diagnostic et les suggestions du Doctor.
**Utilisation** : Prend actuellement en charge l'anglais, le chinois traditionnel, le chinois simplifi√© et le japonais (d'autres √† venir). Les changements s'appliquent aux nouvelles erreurs.

### 5. Enable Doctor (requires restart)

**Fonction** : Interrupteur principal pour le syst√®me d'interception de logs.
**Utilisation** : D√©sactivez pour d√©sactiver compl√®tement la fonctionnalit√© principale de Doctor (n√©cessite un red√©marrage de ComfyUI).

### 6. AI Provider

**Fonction** : S√©lectionnez votre fournisseur de services LLM pr√©f√©r√© dans un menu d√©roulant.
**Options** : OpenAI, DeepSeek, Groq Cloud, Google Gemini, xAI Grok, OpenRouter, Ollama (Local), LMStudio (Local), Personnalis√©.
**Utilisation** : La s√©lection d'un fournisseur remplit automatiquement l'URL de base appropri√©e. Pour les fournisseurs locaux (Ollama/LMStudio), une alerte affiche les mod√®les disponibles.

### 7. AI Base URL

**Fonction** : Le point de terminaison API pour votre service LLM.
**Utilisation** : Rempli automatiquement lorsque vous s√©lectionnez un fournisseur, mais peut √™tre personnalis√© pour les points de terminaison auto-h√©berg√©s ou personnalis√©s.

### 8. AI API Key

**Fonction** : Votre cl√© API pour l'authentification avec les services LLM cloud.
**Utilisation** : Requis pour les fournisseurs de cloud (OpenAI, DeepSeek, etc.). Laissez vide pour les LLM locaux (Ollama, LMStudio).
**S√©curit√©** : La cl√© est transmise uniquement lors des demandes d'analyse et n'est jamais enregistr√©e ou persist√©e.

### 9. AI Model Name

**Fonction** : Sp√©cifiez quel mod√®le utiliser pour l'analyse des erreurs.
**Utilisation** :

- **Mode liste d√©roulante** (d√©faut) : S√©lectionnez un mod√®le dans la liste automatiquement peupl√©e. Cliquez sur le bouton d'actualisation üîÑ pour recharger les mod√®les disponibles.
- **Mode saisie manuelle** : Cochez "Saisir le nom du mod√®le manuellement" pour taper un nom de mod√®le personnalis√© (ex. `gpt-4o`, `deepseek-chat`, `llama3.1:8b`).
- Les mod√®les sont automatiquement r√©cup√©r√©s depuis l'API de votre fournisseur s√©lectionn√© lorsque vous changez de fournisseur ou cliquez sur actualiser.
- Pour les LLM locaux (Ollama/LMStudio), la liste d√©roulante affiche tous les mod√®les disponibles localement.

> Remarque : **Confiance et Sant√© (Trust & Health)** et **T√©l√©m√©trie Anonyme (Anonymous Telemetry)** ont √©t√© d√©plac√©s vers l'onglet **Statistiques (Statistics)**.

> Remarque : **F14 Diagnostics Proactifs (Proactive Diagnostics)** est accessible depuis l'onglet **Statistiques (Statistics)** ‚Üí section **Diagnostics (Diagnostics)**.
> Utilisez **Run / Refresh** pour g√©n√©rer un rapport, afficher la liste des probl√®mes et utiliser les actions fournies (comme localiser le n≈ìud).
> Si vous avez besoin d'afficher le rapport dans une autre langue, modifiez d'abord la **Suggestion Language** dans les param√®tres.

---

## Points de terminaison API

### GET `/debugger/last_analysis`

R√©cup√©rer l'analyse d'erreur la plus r√©cente :

```bash
curl http://localhost:8188/debugger/last_analysis
```

**Exemple de r√©ponse** :

```json
{
  "status": "running",
  "log_path": ".../logs/comfyui_debug_2025-12-28.log",
  "language": "zh_TW",
  "supported_languages": ["en", "zh_TW", "zh_CN", "ja"],
  "last_error": "Traceback...",
  "suggestion": "SUGGESTION : ...",
  "timestamp": "2025-12-28T06:49:11",
  "node_context": {
    "node_id": "42",
    "node_name": "KSampler",
    "node_class": "KSamplerNode",
    "custom_node_path": "ComfyUI-Impact-Pack"
  }
}
```

### GET `/debugger/history`

R√©cup√©rer l'historique des erreurs (20 derni√®res entr√©es) :

```bash
curl http://localhost:8188/debugger/history
```

### POST `/debugger/set_language`

Changer la langue de suggestion (voir la section Changement de langue).

### POST `/doctor/analyze`

Analyser une erreur en utilisant le service LLM configur√©.

**Charge utile** :

```json
{
  "error": "Traceback...",
  "node_context": {...},
  "api_key": "your-api-key",
  "base_url": "https://api.openai.com/v1",
  "model": "gpt-4o",
  "language": "en"
}
```

**R√©ponse** :

```json
{
  "analysis": "AI-generated debugging suggestions..."
}
```

### POST `/doctor/verify_key`

V√©rifier la validit√© de la cl√© API en testant la connexion au fournisseur LLM.

**Charge utile** :

```json
{
  "base_url": "https://api.openai.com/v1",
  "api_key": "your-api-key"
}
```

**R√©ponse** :

```json
{
  "success": true,
  "message": "API key is valid",
  "is_local": false
}
```

### POST `/doctor/list_models`

Lister les mod√®les disponibles du fournisseur LLM configur√©.

**Charge utile** :

```json
{
  "base_url": "http://localhost:11434/v1",
  "api_key": ""
}
```

**R√©ponse** :

```json
{
  "success": true,
  "models": [
    {"id": "llama3.1:8b", "name": "llama3.1:8b"},
    {"id": "mistral:7b", "name": "mistral:7b"}
  ],
  "message": "Found 2 models"
}
```

---

## Fichiers journaux

Tous les journaux sont stock√©s dans :

```text
<ComfyUI user directory>/ComfyUI-Doctor/logs/
```

Format du nom de fichier : `comfyui_debug_YYYY-MM-DD_HH-MM-SS.log`

Le syst√®me conserve automatiquement les 10 fichiers journaux les plus r√©cents (configurable via `config.json`).

---

## Configuration

Cr√©ez `config.json` pour personnaliser le comportement :

```json
{
  "max_log_files": 10,
  "buffer_limit": 100,
  "traceback_timeout_seconds": 5.0,
  "history_size": 20,
  "default_language": "zh_TW",
  "enable_api": true,
  "privacy_mode": "basic"
}
```

**Param√®tres** :

- `max_log_files` : Nombre maximum de fichiers journaux √† conserver
- `buffer_limit` : Taille du tampon de traceback (nombre de lignes)
- `traceback_timeout_seconds` : D√©lai d'attente pour les tracebacks incomplets
- `history_size` : Nombre d'erreurs √† conserver dans l'historique
- `default_language` : Langue de suggestion par d√©faut
- `enable_api` : Activer les points de terminaison API
- `privacy_mode` : Niveau de d√©sinfection PII - `"none"`, `"basic"` (d√©faut), ou `"strict"`

---

## Motifs d'erreur pris en charge

ComfyUI-Doctor peut d√©tecter et fournir des suggestions pour :

- Inad√©quations de type (ex. fp16 vs float32)
- Inad√©quations de dimension
- M√©moire CUDA/MPS insuffisante (OOM)
- Erreurs de multiplication matricielle
- Conflits p√©riph√©rique/type
- Modules Python manquants
- √âchecs d'assertion
- Erreurs Cl√©/Attribut
- Inad√©quations de forme (Shape mismatches)
- Erreurs de fichier non trouv√©
- Erreurs de chargement SafeTensors
- √âchecs d'ex√©cution CUDNN
- Biblioth√®que InsightFace manquante
- Inad√©quations Mod√®le/VAE
- JSON de prompt invalide

Et plus encore...

---

## Conseils

1. **Associer avec ComfyUI Manager** : Installez automatiquement les n≈ìuds personnalis√©s manquants
2. **V√©rifier les fichiers journaux** : Les traces compl√®tes sont enregistr√©es pour le signalement des probl√®mes
3. **Utiliser la barre lat√©rale int√©gr√©e** : Cliquez sur l'ic√¥ne üè• Doctor dans le menu de gauche pour des diagnostics en temps r√©el
4. **D√©bogage de n≈ìud** : Connectez les n≈ìuds de d√©bogage pour inspecter le flux de donn√©es suspect

---

## Licence

Licence MIT

---

## Contribuer

Les contributions sont les bienvenues ! N'h√©sitez pas √† soumettre une Pull Request.

**Signaler des probl√®mes** : Vous avez trouv√© un bug ou avez une suggestion ? Ouvrez un ticket (issue) sur GitHub.
**Soumettre des PR** : Aidez √† am√©liorer la base de code avec des corrections de bugs ou des am√©liorations g√©n√©rales.
**Demandes de fonctionnalit√©s** : Vous avez des id√©es de nouvelles fonctionnalit√©s ? Faites-le nous savoir s'il vous pla√Æt.
